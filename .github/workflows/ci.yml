name: DNS Toolkit Daily Processing

# 1. cache-only: Download and update cache only (no processing/publishing)
# 2. use-cache: Use existing cache, skip download, run full pipeline (exit if no cache)
# 3. full-pipeline: Download fresh data, run full pipeline, publish (default)

on:
  schedule:
    - cron: '0 14 * * *'  # Daily at 2 PM UTC
  workflow_dispatch:
    inputs:
      mode:
        description: 'Pipeline mode'
        required: false
        default: 'full-pipeline'
        type: choice
        options:
          - 'full-pipeline'
          - 'cache-only'
          - 'use-cache'
  push:
    branches: [main]
  pull_request:
    branches: [main]

permissions:
  contents: read

concurrency:
  group: dns-toolkit-pipeline
  cancel-in-progress: false
  
env:
  GO_VERSION: "1.23"
  COVERAGE_THRESHOLD: 80
  GOLANGCI_LINT_VERSION: v2.1.6

jobs:
  test-and-build:
    name: Test and Build
    runs-on: ubuntu-latest
    outputs:
      should-publish: ${{ steps.check-publish.outputs.should-publish }}
      pipeline-mode: ${{ steps.check-publish.outputs.pipeline-mode }}
    steps:
      - name: Checkout code
        uses: actions/checkout@v4

      - name: Set up Go
        uses: actions/setup-go@v5
        with:
          go-version: ${{ env.GO_VERSION }}
          cache: true

      - name: Install golangci-lint
        run: |
          curl -sSfL https://raw.githubusercontent.com/golangci/golangci-lint/master/install.sh | sh -s -- -b $(go env GOPATH)/bin ${{ env.GOLANGCI_LINT_VERSION }}

      - name: Cache lint results
        uses: actions/cache@v4
        with:
          path: ~/.cache/golangci-lint
          key: ${{ runner.os }}-golangci-lint-${{ hashFiles('.golangci.yml', 'go.sum') }}
          restore-keys: |
            ${{ runner.os }}-golangci-lint-

      - name: Lint and test
        env:
          DNS_TOOLKIT_TEST_MODE: true
          DNS_TOOLKIT_TEST_CONFIG_PATH: ${{ github.workspace }}/testdata/config.yml
        run: |
          echo "Running linter..."
          golangci-lint run ./... --timeout=5m
          
          echo "Building application..."
          go build ./...
          
          echo "Running tests with coverage..."
          PACKAGES=$(go list ./... | grep -v "/mocks" | grep -v "constants")
          go test -v -race -coverprofile=coverage.out -covermode=atomic $PACKAGES
          
          # Filter out test helpers
          if [ -f coverage.out ]; then
            echo "Filtering coverage report..."
            grep -v "test_helpers.go" coverage.out > filtered_coverage.out
            mv filtered_coverage.out coverage.out
          fi

      - name: Check coverage
        run: |
          if [ -f coverage.out ]; then
            COVERAGE=$(go tool cover -func=coverage.out | grep total: | awk '{print $3}' | sed 's/%//')
            echo "Coverage: ${COVERAGE}%"
            if (( $(echo "$COVERAGE < $COVERAGE_THRESHOLD" | bc -l) )); then
              echo "::error::Coverage ${COVERAGE}% is below threshold ${COVERAGE_THRESHOLD}%"
              exit 1
            fi
            echo "::notice::Coverage check passed: ${COVERAGE}%"
          else
            echo "::warning::No coverage file found"
          fi

      - name: Upload coverage
        uses: codecov/codecov-action@v4
        with:
          file: ./coverage.out
          flags: unittests
          name: codecov-umbrella
          fail_ci_if_error: false
          verbose: false
          override_commit: ${{ github.sha }}

      - name: Determine pipeline mode
        id: check-publish
        run: |
          # Determine pipeline mode based on event and input
          if [[ "${{ github.event_name }}" == "pull_request" ]]; then
            MODE="test-only"
            PUBLISH="false"
            echo "::notice::Pull request - tests only"
          elif [[ "${{ github.event_name }}" == "workflow_dispatch" ]]; then
            MODE="${{ github.event.inputs.mode }}"
            PUBLISH="true"
            echo "::notice::Manual dispatch - Mode: $MODE"
          elif [[ "${{ github.event_name }}" == "schedule" ]] || 
               [[ "${{ github.event_name }}" == "push" && "${{ github.ref }}" == "refs/heads/main" ]]; then
            MODE="full-pipeline"
            PUBLISH="true"
            echo "::notice::Scheduled/push event - Mode: $MODE"
          else
            MODE="test-only"
            PUBLISH="false"
            echo "::notice::Other event - tests only"
          fi
          
          echo "pipeline-mode=$MODE" >> $GITHUB_OUTPUT
          echo "should-publish=$PUBLISH" >> $GITHUB_OUTPUT

  publish:
    name: DNS Toolkit Pipeline
    runs-on: ubuntu-latest
    needs: test-and-build
    if: needs.test-and-build.outputs.should-publish == 'true'
    timeout-minutes: 20
    permissions:
      contents: write
      actions: read
    env:
      PIPELINE_MODE: ${{ needs.test-and-build.outputs.pipeline-mode }}
    steps:
      - name: Checkout code
        uses: actions/checkout@v4
        with:
          token: ${{ secrets.GITHUB_TOKEN }}
          fetch-depth: 0
          persist-credentials: true

      - name: Configure Git
        run: |
          git config user.name "GitHub Actions Bot"
          git config user.email "actions@github.com"
          git config pull.rebase false
          git config push.default simple

      - name: Set up Go
        uses: actions/setup-go@v5
        with:
          go-version: ${{ env.GO_VERSION }}
          cache: true

      - name: Build DNS toolkit
        run: |
          echo "Building DNS toolkit..."
          go build -o bin/dns-toolkit .
          chmod +x bin/dns-toolkit
          mkdir -p data/{download,output}
          echo "Build completed successfully"

      - name: Display pipeline mode
        run: |
          echo "## Pipeline Mode: ${{ env.PIPELINE_MODE }}" >> $GITHUB_STEP_SUMMARY
          case "${{ env.PIPELINE_MODE }}" in
            "cache-only")
              echo "- **Goal**: Download and update cache only" >> $GITHUB_STEP_SUMMARY
              echo "- **Actions**: Download fresh data, save to cache" >> $GITHUB_STEP_SUMMARY
              echo "- **Skip**: Processing and publishing" >> $GITHUB_STEP_SUMMARY
              ;;
            "use-cache")
              echo "- **Goal**: Use existing cache, run full pipeline" >> $GITHUB_STEP_SUMMARY
              echo "- **Actions**: Restore cache, skip download, process, publish" >> $GITHUB_STEP_SUMMARY
              echo "- **Requirement**: Cache must exist or pipeline will fail" >> $GITHUB_STEP_SUMMARY
              ;;
            "full-pipeline")
              echo "- **Goal**: Complete pipeline with fresh data" >> $GITHUB_STEP_SUMMARY
              echo "- **Actions**: Download fresh data, process, publish, save cache" >> $GITHUB_STEP_SUMMARY
              echo "- **Note**: This is the default mode" >> $GITHUB_STEP_SUMMARY
              ;;
          esac

      - name: Restore cache
        if: env.PIPELINE_MODE == 'use-cache' || env.PIPELINE_MODE == 'full-pipeline'
        uses: actions/cache/restore@v4
        id: cache-restore
        with:
          path: |
            data/download
            data/download_summary.json
          key: data-download-${{ runner.os }}-${{ hashFiles('data/config/sources*.json') }}-v3
          restore-keys: |
            data-download-${{ runner.os }}-${{ hashFiles('data/config/sources*.json') }}-
            data-download-${{ runner.os }}-

      - name: Validate cache for use-cache mode
        if: env.PIPELINE_MODE == 'use-cache'
        run: |
          echo "Validating cache availability..."
          
          if [[ "${{ steps.cache-restore.outputs.cache-hit }}" == "true" ]]; then
            echo "✅ Cache hit: ${{ steps.cache-restore.outputs.cache-matched-key }}"
          elif [[ "${{ steps.cache-restore.outputs.cache-hit }}" != "true" && -d "data/download" ]]; then
            echo "⚠️  Partial cache restored from: ${{ steps.cache-restore.outputs.cache-matched-key }}"
          else
            echo "❌ No cache available"
            echo "::error::use-cache mode requires existing cache but none found"
            exit 1
          fi
          
          # Check cache contents
          if [[ -d "data/download" ]]; then
            CACHED_FILES=$(find data/download -type f 2>/dev/null | wc -l)
            if [[ "$CACHED_FILES" -gt 0 ]]; then
              echo "Cache contains: $CACHED_FILES files"
              if [[ -f "data/download/.cache_timestamp" ]]; then
                CACHE_TIME=$(cat data/download/.cache_timestamp)
                echo "Cache timestamp: $CACHE_TIME"
              fi
            else
              echo "::error::Cache directory exists but is empty"
              exit 1
            fi
          else
            echo "::error::Cache directory not found"
            exit 1
          fi

      - name: Cache-only pipeline
        if: env.PIPELINE_MODE == 'cache-only'
        run: |
          echo "Running cache-only pipeline..."
          echo "Downloading fresh data to update cache..."
          
          # Check existing cache first
          if [[ -d "data/download" && "$(find data/download -type f 2>/dev/null | wc -l)" -gt 0 ]]; then
            EXISTING_FILES=$(find data/download -type f 2>/dev/null | wc -l)
            echo "Existing cache contains: $EXISTING_FILES files"
          else
            echo "No existing cache found"
          fi
          
          # Download fresh data
          ./bin/dns-toolkit download
          
          # Update cache timestamp
          echo "$(date -u '+%Y-%m-%d %H:%M:%S UTC')" > data/download/.cache_timestamp
          
          # Report results
          UPDATED_FILES=$(find data/download -type f 2>/dev/null | wc -l)
          echo "✅ Cache updated with: $UPDATED_FILES files"
          echo "Cache timestamp: $(cat data/download/.cache_timestamp)"
          
          # Add to summary
          echo "### Cache Update Results" >> $GITHUB_STEP_SUMMARY
          echo "- **Files downloaded**: $UPDATED_FILES" >> $GITHUB_STEP_SUMMARY
          echo "- **Cache timestamp**: $(cat data/download/.cache_timestamp)" >> $GITHUB_STEP_SUMMARY
          echo "- **Status**: ✅ Cache updated successfully" >> $GITHUB_STEP_SUMMARY

      - name: Full pipeline
        if: env.PIPELINE_MODE == 'use-cache' || env.PIPELINE_MODE == 'full-pipeline'
        run: |
          echo "🚀 Running full DNS toolkit pipeline..."
          
          # Step 1: Download (only for full-pipeline mode)
          if [[ "${{ env.PIPELINE_MODE }}" == "full-pipeline" ]]; then
            echo "Step 1: Downloading fresh data..."
            ./bin/dns-toolkit download
            echo "$(date -u '+%Y-%m-%d %H:%M:%S UTC')" > data/download/.cache_timestamp
            
            DOWNLOAD_FILES=$(find data/download -type f 2>/dev/null | wc -l)
            echo "Downloaded: $DOWNLOAD_FILES files"
          else
            echo "Step 1: Using cached data (skipping download)"
            DOWNLOAD_FILES=$(find data/download -type f 2>/dev/null | wc -l)
            echo "Using cache: $DOWNLOAD_FILES files"
          fi
          
          # Processing pipeline
          echo "Step 2: Processing data..."
          ./bin/dns-toolkit process
          
          echo "Step 3: Consolidating data..."
          ./bin/dns-toolkit consolidate

          echo "Step 4: Consolidating groups..."
          ./bin/dns-toolkit consolidate groups

          echo "Step 5: Consolidating categories..."
          ./bin/dns-toolkit consolidate categories
          
          echo "Step 6: Generating top lists..."
          ./bin/dns-toolkit top
          
          echo "Step 7: Calculating overlaps..."
          ./bin/dns-toolkit overlap
          
          echo "Step 8: Generating output..."
          ./bin/dns-toolkit generate output -i

          echo "Step 9: Generating output README..."
          ./bin/dns-toolkit generate output-readme
          
          echo "Step 10: Generating overlap README..."
          ./bin/dns-toolkit generate overlap-readme
          
          echo "Step 11: Generating summaries README..."
          ./bin/dns-toolkit generate summaries-readme
          
          echo "✅ Pipeline completed successfully"

      - name: Count and validate output files
        if: env.PIPELINE_MODE == 'use-cache' || env.PIPELINE_MODE == 'full-pipeline'
        id: count-files
        run: |
          OUTPUT_COUNT=$(find data/output -type f ! -path "*/summaries/*" 2>/dev/null | wc -l)
          SUMMARY_COUNT=$(find data/output/summaries -name "*.json" 2>/dev/null | wc -l)
          DOWNLOAD_COUNT=$(find data/download -type f 2>/dev/null | wc -l)
          
          echo "output-count=$OUTPUT_COUNT" >> $GITHUB_OUTPUT
          echo "summary-count=$SUMMARY_COUNT" >> $GITHUB_OUTPUT
          echo "download-count=$DOWNLOAD_COUNT" >> $GITHUB_OUTPUT
          
          echo "Files generated:"
          echo "  - Output files: $OUTPUT_COUNT"
          echo "  - Summary files: $SUMMARY_COUNT"
          echo "  - Download files: $DOWNLOAD_COUNT"
          
          if [[ "$OUTPUT_COUNT" -eq 0 && "$SUMMARY_COUNT" -eq 0 ]]; then
            echo "::error::No output files generated"
            exit 1
          fi

      - name: Publish results
        if: env.PIPELINE_MODE == 'use-cache' || env.PIPELINE_MODE == 'full-pipeline'
        run: |
          echo "Publishing results..."
          
          # Create temporary directories
          mkdir -p /tmp/publish-data/{output,summaries,download}
          
          # Backup download directory and summary file for caching
          if [[ -d "data/download" ]]; then
            cp -r data/download/* /tmp/publish-data/download/ 2>/dev/null || true
          fi
          
          if [[ -f "data/download_summary.json" ]]; then
            cp data/download_summary.json /tmp/publish-data/download_summary.json
          fi
          
          # Prepare output and summary files
          if [[ -d "data/output" ]]; then
            rsync -av --exclude='summaries/' data/output/ /tmp/publish-data/output/ 2>/dev/null || true
          fi
          
          if [[ -d "data/output/summaries" ]]; then
            cp -r data/output/summaries/* /tmp/publish-data/summaries/ 2>/dev/null || true
          fi
          
          # Store original branch and prepare publish script
          echo "ORIGINAL_BRANCH=$(git rev-parse --abbrev-ref HEAD)" >> $GITHUB_ENV
          chmod +x .github/scripts/publish-to-branch.sh
          cp .github/scripts/publish-to-branch.sh /tmp/publish-to-branch.sh
          
          # Publish to branches
          if [[ "${{ steps.count-files.outputs.output-count }}" -gt 0 ]]; then
            echo "Publishing to output branch..."
            GITHUB_WORKSPACE="${{ github.workspace }}" /tmp/publish-to-branch.sh output /tmp/publish-data/output
          fi
          
          if [[ "${{ steps.count-files.outputs.summary-count }}" -gt 0 ]]; then
            echo "Publishing to summaries branch..."
            GITHUB_WORKSPACE="${{ github.workspace }}" /tmp/publish-to-branch.sh summaries /tmp/publish-data/summaries --monthly
          fi
          
          # Restore original branch and download data
          git checkout "$ORIGINAL_BRANCH" 2>/dev/null || true
          
          if [[ -d "/tmp/publish-data/download" ]]; then
            mkdir -p data/download
            cp -r /tmp/publish-data/download/* data/download/ 2>/dev/null || true
          fi
          
          if [[ -f "/tmp/publish-data/download_summary.json" ]]; then
            cp /tmp/publish-data/download_summary.json data/download_summary.json
          fi
          
          echo "✅ Publishing completed"

      - name: Save cache
        if: env.PIPELINE_MODE == 'cache-only' || env.PIPELINE_MODE == 'full-pipeline'
        uses: actions/cache/save@v4
        with:
          path: |
            data/download
            data/download_summary.json
          key: data-download-${{ runner.os }}-${{ hashFiles('data/config/sources*.json') }}-v3

      - name: Final summary
        if: always()
        run: |
          # Cleanup temporary files
          rm -rf /tmp/publish-data/ /tmp/publish-to-branch.sh 2>/dev/null || true
          
          # Generate detailed summary
          echo "## Pipeline Results" >> $GITHUB_STEP_SUMMARY
          echo "- **Mode**: ${{ env.PIPELINE_MODE }}" >> $GITHUB_STEP_SUMMARY
          echo "- **Status**: ${{ job.status }}" >> $GITHUB_STEP_SUMMARY
          echo "- **Completed**: $(date -u '+%Y-%m-%d %H:%M UTC')" >> $GITHUB_STEP_SUMMARY
          echo "" >> $GITHUB_STEP_SUMMARY
          
          case "${{ env.PIPELINE_MODE }}" in
            "cache-only")
              if [[ -f "data/download/.cache_timestamp" ]]; then
                CACHE_FILES=$(find data/download -type f 2>/dev/null | wc -l)
                echo "### Cache Results" >> $GITHUB_STEP_SUMMARY
                echo "- **Files cached**: $CACHE_FILES" >> $GITHUB_STEP_SUMMARY
                echo "- **Cache updated**: $(cat data/download/.cache_timestamp)" >> $GITHUB_STEP_SUMMARY
              fi
              ;;
            "use-cache"|"full-pipeline")
              echo "### File Counts" >> $GITHUB_STEP_SUMMARY
              echo "- **Download files**: ${{ steps.count-files.outputs.download-count }}" >> $GITHUB_STEP_SUMMARY
              echo "- **Output files**: ${{ steps.count-files.outputs.output-count }}" >> $GITHUB_STEP_SUMMARY
              echo "- **Summary files**: ${{ steps.count-files.outputs.summary-count }}" >> $GITHUB_STEP_SUMMARY
              
              if [[ "${{ env.PIPELINE_MODE }}" == "use-cache" ]]; then
                echo "- **Download source**: Existing cache" >> $GITHUB_STEP_SUMMARY
              else
                echo "- **Download source**: Fresh download" >> $GITHUB_STEP_SUMMARY
              fi
              
              if [[ -f "data/download/.cache_timestamp" ]]; then
                echo "- **Cache timestamp**: $(cat data/download/.cache_timestamp)" >> $GITHUB_STEP_SUMMARY
              fi
              ;;
          esac