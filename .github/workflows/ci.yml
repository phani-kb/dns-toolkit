name: DNS Toolkit Daily Processing

# This workflow has two main jobs:
# 1. test-and-build: Always runs for all events (push, PR, schedule, manual)
# 2. publish: Only runs for main branch pushes, scheduled runs, or manual dispatch (NOT for PRs)

on:
  schedule:
    - cron: '0 14 * * *'  # Daily at 2 PM UTC
  workflow_dispatch:
    inputs:
      skip_cache:
        description: 'Skip cache and force fresh download'
        required: false
        default: false
        type: boolean
      skip_download:
        description: 'Skip download step (use existing data)'
        required: false
        default: false
        type: boolean
      dry_run:
        description: 'Run without publishing (dry run)'
        required: false
        default: false
        type: boolean
      download_only:
        description: 'Download only mode - fresh download if no cache, save cache, skip all processing'
        required: false
        default: false
        type: boolean
  push:
    branches: [main]
  pull_request:
    branches: [main]

permissions:
  contents: read

env:
  GO_VERSION: "1.23"
  COVERAGE_THRESHOLD: 80
  GOLANGCI_LINT_VERSION: v2.1.6

jobs:
  test-and-build:
    name: Test and Build
    runs-on: ubuntu-latest
    outputs:
      should-publish: ${{ steps.check-publish.outputs.should-publish }}
    steps:
      - name: Checkout code
        uses: actions/checkout@v4

      - name: Set up Go
        uses: actions/setup-go@v5
        with:
          go-version: ${{ env.GO_VERSION }}
          cache: true

      - name: Install golangci-lint
        run: |
          curl -sSfL https://raw.githubusercontent.com/golangci/golangci-lint/master/install.sh | sh -s -- -b $(go env GOPATH)/bin ${{ env.GOLANGCI_LINT_VERSION }}

      - name: Cache lint results
        uses: actions/cache@v4
        with:
          path: ~/.cache/golangci-lint
          key: ${{ runner.os }}-golangci-lint-${{ hashFiles('.golangci.yml', 'go.sum') }}
          restore-keys: |
            ${{ runner.os }}-golangci-lint-

      - name: Lint and test
        env:
          DNS_TOOLKIT_TEST_MODE: true
          DNS_TOOLKIT_TEST_CONFIG_PATH: ${{ github.workspace }}/testdata/config.yml
        run: |
          echo "Running linter..."
          golangci-lint run ./... --timeout=5m
          
          echo "Building application..."
          go build ./...
          
          echo "Running tests with coverage..."
          PACKAGES=$(go list ./... | grep -v "/mocks" | grep -v "constants")
          go test -v -race -coverprofile=coverage.out -covermode=atomic $PACKAGES
          
          # Filter out test helpers
          if [ -f coverage.out ]; then
            echo "Filtering coverage report..."
            grep -v "test_helpers.go" coverage.out > filtered_coverage.out
            mv filtered_coverage.out coverage.out
          fi

      - name: Check coverage
        run: |
          if [ -f coverage.out ]; then
            COVERAGE=$(go tool cover -func=coverage.out | grep total: | awk '{print $3}' | sed 's/%//')
            echo "Coverage: ${COVERAGE}%"
            if (( $(echo "$COVERAGE < $COVERAGE_THRESHOLD" | bc -l) )); then
              echo "::error::Coverage ${COVERAGE}% is below threshold ${COVERAGE_THRESHOLD}%"
              exit 1
            fi
            echo "::notice::Coverage check passed: ${COVERAGE}%"
          else
            echo "::warning::No coverage file found"
          fi

      - name: Upload coverage
        uses: codecov/codecov-action@v4
        with:
          file: ./coverage.out
          flags: unittests
          name: codecov-umbrella
          fail_ci_if_error: false
          verbose: false
          override_commit: ${{ github.sha }}

      - name: Check if should publish
        id: check-publish
        run: |
          # Determine publishing mode
          if [[ "${{ github.event.inputs.download_only }}" == "true" ]]; then
            echo "should-publish=download-only" >> $GITHUB_OUTPUT
            echo "::notice::Download only mode enabled"
          elif [[ "${{ github.event.inputs.dry_run }}" == "true" ]]; then
            echo "should-publish=false" >> $GITHUB_OUTPUT
            echo "::notice::Dry run mode - skipping publish"
          elif [[ "${{ github.event_name }}" == "pull_request" ]]; then
            echo "should-publish=false" >> $GITHUB_OUTPUT
            echo "::notice::Pull request - tests only"
          elif [[ "${{ github.event_name }}" == "push" && "${{ github.ref }}" == "refs/heads/main" ]] || 
               [[ "${{ github.event_name }}" == "schedule" ]] || 
               [[ "${{ github.event_name }}" == "workflow_dispatch" ]]; then
            echo "should-publish=true" >> $GITHUB_OUTPUT
            echo "::notice::Will publish - Event: ${{ github.event_name }}"
          else
            echo "should-publish=false" >> $GITHUB_OUTPUT
            echo "::notice::Not a publishing event"
          fi

  publish:
    name: Download, Process and Publish
    runs-on: ubuntu-latest
    needs: test-and-build
    if: needs.test-and-build.outputs.should-publish == 'true' || needs.test-and-build.outputs.should-publish == 'download-only'
    timeout-minutes: 20
    permissions:
      contents: write
      actions: read
    env:
      DOWNLOAD_ONLY: ${{ needs.test-and-build.outputs.should-publish == 'download-only' }}
    steps:
      - name: Checkout code
        uses: actions/checkout@v4
        with:
          token: ${{ secrets.GITHUB_TOKEN }}
          fetch-depth: 0
          persist-credentials: true

      - name: Configure Git
        run: |
          git config user.name "GitHub Actions Bot"
          git config user.email "actions@github.com"
          git config pull.rebase false
          git config push.default simple

      - name: Set up Go
        uses: actions/setup-go@v5
        with:
          go-version: ${{ env.GO_VERSION }}
          cache: true

      - name: Restore cached data
        if: github.event.inputs.skip_cache != 'true'
        uses: actions/cache/restore@v4
        id: cache-restore
        with:
          path: |
            data/download
            data/download_summary.json
          key: data-download-${{ runner.os }}-${{ hashFiles('data/config/sources*.json') }}-v3
          restore-keys: |
            data-download-${{ runner.os }}-${{ hashFiles('data/config/sources*.json') }}-
            data-download-${{ runner.os }}-

      - name: Build DNS toolkit
        run: |
          echo "Building DNS toolkit..."
          go build -o bin/dns-toolkit .
          chmod +x bin/dns-toolkit
          mkdir -p data/{download,output}
          echo "Build completed successfully"

      - name: Determine download strategy
        id: download-strategy
        run: |
          SKIP_DOWNLOAD="false"
          
          if [[ "${{ env.DOWNLOAD_ONLY }}" == "true" ]]; then
            echo "::notice::Download-only mode - will always download fresh data to update cache"
            SKIP_DOWNLOAD="false"
          elif [[ "${{ github.event.inputs.skip_download }}" == "true" ]]; then
            echo "::notice::User requested to skip download"
            SKIP_DOWNLOAD="true"
          elif [[ "${{ github.event.inputs.skip_cache }}" == "true" ]]; then
            echo "::notice::User requested to skip cache - downloading fresh"
            SKIP_DOWNLOAD="false"
          elif [[ "${{ github.event_name }}" == "schedule" ]] || 
               [[ "${{ github.event_name }}" == "workflow_dispatch" ]]; then
            echo "::notice::Scheduled/manual run - downloading fresh data"
            SKIP_DOWNLOAD="false"
          else
            echo "::notice::Using cached data only"
            SKIP_DOWNLOAD="true"
          fi
          
          echo "skip-download=$SKIP_DOWNLOAD" >> $GITHUB_OUTPUT
          
          # Validate cache availability if skipping download (only for full pipeline)
          if [[ "$SKIP_DOWNLOAD" == "true" && "${{ env.DOWNLOAD_ONLY }}" != "true" ]]; then
            if [[ ! -f "data/download_summary.json" ]] || [[ ! -d "data/download" ]]; then
              echo "::error::Cannot proceed without download data when skipping download"
              exit 1
            fi
          fi

      - name: Run download-only pipeline
        if: env.DOWNLOAD_ONLY == 'true'
        run: |
          echo "Running download-only pipeline..."
          
          # Check if we have existing cache
          if [[ -d "data/download" && "$(find data/download -type f 2>/dev/null | wc -l)" -gt 0 ]]; then
            echo "Existing cache found - will update with fresh data"
            CACHED_FILES=$(find data/download -type f 2>/dev/null | wc -l)
            echo "Current cache contains: $CACHED_FILES files"
          else
            echo "No existing cache - will create fresh cache"
          fi
          
          # Always download fresh data in download-only mode to update cache
          echo "Downloading fresh data to update cache..."
          ./bin/dns-toolkit download
          
          # Update cache timestamp
          echo "$(date -u '+%Y-%m-%d %H:%M:%S UTC')" > data/download/.cache_timestamp
          
          # Verify updated cache
          UPDATED_FILES=$(find data/download -type f 2>/dev/null | wc -l)
          echo "Updated cache now contains: $UPDATED_FILES files"
          echo "::notice::Download-only mode completed - cache updated with fresh data"

      - name: Run full DNS toolkit pipeline
        if: env.DOWNLOAD_ONLY != 'true'
        run: |
          echo "Starting DNS toolkit pipeline..."
          
          # Conditional download step
          if [[ "${{ steps.download-strategy.outputs.skip-download }}" == "true" ]]; then
            echo "Step 1: Skipping download (using cached data)..."
          else
            echo "Step 1: Downloading data..."
            ./bin/dns-toolkit download
            echo "$(date -u '+%Y-%m-%d %H:%M:%S UTC')" > data/download/.cache_timestamp
          fi
          
          # Processing pipeline
          echo "Step 2: Processing data..."
          ./bin/dns-toolkit process
          
          echo "Step 3: Consolidating data..."
          ./bin/dns-toolkit consolidate
          
          echo "Step 4: Consolidating groups..."
          ./bin/dns-toolkit consolidate groups
          
          echo "Step 5: Consolidating categories..."
          ./bin/dns-toolkit consolidate categories
          
          echo "Step 6: Generating top lists..."
          ./bin/dns-toolkit top
          
          echo "Step 7: Calculating overlaps..."
          ./bin/dns-toolkit overlap
          
          echo "Step 8: Generating output..."
          ./bin/dns-toolkit generate output -i
          
          echo "Step 9: Generating output README..."
          ./bin/dns-toolkit generate output-readme
          
          echo "Step 10: Generating overlap README..."
          ./bin/dns-toolkit generate overlap-readme
          
          echo "Step 11: Generating summaries README..."
          ./bin/dns-toolkit generate summaries-readme
          
          echo "DNS toolkit pipeline completed successfully"

      - name: Count and validate output files
        if: env.DOWNLOAD_ONLY != 'true'
        id: count-files
        run: |
          OUTPUT_COUNT=$(find data/output -type f ! -path "*/summaries/*" 2>/dev/null | wc -l)
          SUMMARY_COUNT=$(find data/output/summaries -name "*.json" 2>/dev/null | wc -l)
          
          echo "output-count=$OUTPUT_COUNT" >> $GITHUB_OUTPUT
          echo "summary-count=$SUMMARY_COUNT" >> $GITHUB_OUTPUT
          
          echo "::notice::Output files: $OUTPUT_COUNT"
          echo "::notice::Summary files: $SUMMARY_COUNT"
          
          if [[ "$OUTPUT_COUNT" -eq 0 && "$SUMMARY_COUNT" -eq 0 ]]; then
            echo "::error::No output files generated"
            exit 1
          fi

      - name: Prepare and publish files
        if: env.DOWNLOAD_ONLY != 'true'
        run: |
          echo "Preparing files for publishing..."
          
          # Create temporary directories
          mkdir -p /tmp/publish-data/{output,summaries,download}
          
          # Backup download directory and summary file for caching
          if [[ -d "data/download" ]]; then
            cp -r data/download/* /tmp/publish-data/download/ 2>/dev/null || true
          fi
          
          if [[ -f "data/download_summary.json" ]]; then
            cp data/download_summary.json /tmp/publish-data/download_summary.json
          fi
          
          # Prepare output and summary files
          if [[ -d "data/output" ]]; then
            rsync -av --exclude='summaries/' data/output/ /tmp/publish-data/output/ 2>/dev/null || true
          fi
          
          if [[ -d "data/output/summaries" ]]; then
            cp -r data/output/summaries/* /tmp/publish-data/summaries/ 2>/dev/null || true
          fi
          
          # Store original branch and prepare publish script
          echo "ORIGINAL_BRANCH=$(git rev-parse --abbrev-ref HEAD)" >> $GITHUB_ENV
          chmod +x .github/scripts/publish-to-branch.sh
          cp .github/scripts/publish-to-branch.sh /tmp/publish-to-branch.sh
          
          # Publish to branches
          if [[ "${{ steps.count-files.outputs.output-count }}" -gt 0 ]]; then
            echo "Publishing to output branch..."
            GITHUB_WORKSPACE="${{ github.workspace }}" /tmp/publish-to-branch.sh output /tmp/publish-data/output
          fi
          
          if [[ "${{ steps.count-files.outputs.summary-count }}" -gt 0 ]]; then
            echo "Publishing to summaries branch..."
            GITHUB_WORKSPACE="${{ github.workspace }}" /tmp/publish-to-branch.sh summaries /tmp/publish-data/summaries --monthly
          fi
          
          # Restore original branch
          git checkout "$ORIGINAL_BRANCH" 2>/dev/null || true
          
          # Restore download directory and summary file for caching
          if [[ -d "/tmp/publish-data/download" ]]; then
            mkdir -p data/download
            cp -r /tmp/publish-data/download/* data/download/ 2>/dev/null || true
          fi
          
          if [[ -f "/tmp/publish-data/download_summary.json" ]]; then
            cp /tmp/publish-data/download_summary.json data/download_summary.json
          fi

      - name: Save cache for next run
        if: github.event.inputs.skip_cache != 'true' && (hashFiles('data/download/**') != '' || hashFiles('data/download_summary.json') != '')
        uses: actions/cache/save@v4
        with:
          path: |
            data/download
            data/download_summary.json
          key: data-download-${{ runner.os }}-${{ hashFiles('data/config/sources*.json') }}-v3

      - name: Cleanup and summary
        if: always()
        run: |
          # Cleanup temporary files
          rm -rf /tmp/publish-data/ /tmp/publish-to-branch.sh 2>/dev/null || true
          
          # Generate summary
          echo "## Workflow Summary" >> $GITHUB_STEP_SUMMARY
          
          if [[ "${{ env.DOWNLOAD_ONLY }}" == "true" ]]; then
            echo "- **Mode**: Download Only" >> $GITHUB_STEP_SUMMARY
            echo "- **Download**: Fresh download completed (cache updated)" >> $GITHUB_STEP_SUMMARY
            echo "- **Processing**: Skipped (download-only mode)" >> $GITHUB_STEP_SUMMARY
          else
            echo "- **Mode**: Full Pipeline" >> $GITHUB_STEP_SUMMARY
            echo "- **Output files published**: ${{ steps.count-files.outputs.output-count }}" >> $GITHUB_STEP_SUMMARY
            echo "- **Summary files published**: ${{ steps.count-files.outputs.summary-count }}" >> $GITHUB_STEP_SUMMARY
          fi
          
          echo "- **Workflow status**: ${{ job.status }}" >> $GITHUB_STEP_SUMMARY
          echo "- **Completed at**: $(date -u '+%Y-%m-%d %H:%M UTC')" >> $GITHUB_STEP_SUMMARY